{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用sparkmagic与spark进行交互\n",
    "\n",
    "\n",
    "使用静态数据做分析的时候我们更加希望spark作为我们的计算资源,在远程集群上让我们作为运算资源调用,运算完成后可能还需要做个可视化的工作.这种时候就可以使用Livy配合sparkmagic,借助jupyter notebook来与spark交互了.注意这种方式一般用于分析静态数据,好处是可以充分利用本地的python包.\n",
    "\n",
    "\n",
    "一般来说只要在master机器上启动Livy的服务,同时其8998端口是对外的,那么就可以在本地的`~/.sparkmagic/config.json`中修改配置连接Livy.下面是默认配置:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"kernel_python_credentials\" : {\n",
    "    \"username\": \"\",\n",
    "    \"password\": \"\",\n",
    "    \"url\": \"http://localhost:8998\",\n",
    "    \"auth\": \"None\"\n",
    "  },\n",
    "\n",
    "  \"kernel_scala_credentials\" : {\n",
    "    \"username\": \"\",\n",
    "    \"password\": \"\",\n",
    "    \"url\": \"http://localhost:8998\",\n",
    "    \"auth\": \"None\"\n",
    "  },\n",
    "  \"kernel_r_credentials\": {\n",
    "    \"username\": \"\",\n",
    "    \"password\": \"\",\n",
    "    \"url\": \"http://localhost:8998\"\n",
    "  },\n",
    "\n",
    "  \"logging_config\": {\n",
    "    \"version\": 1,\n",
    "    \"formatters\": {\n",
    "      \"magicsFormatter\": { \n",
    "        \"format\": \"%(asctime)s\\t%(levelname)s\\t%(message)s\",\n",
    "        \"datefmt\": \"\"\n",
    "      }\n",
    "    },\n",
    "    \"handlers\": {\n",
    "      \"magicsHandler\": { \n",
    "        \"class\": \"hdijupyterutils.filehandler.MagicsFileHandler\",\n",
    "        \"formatter\": \"magicsFormatter\",\n",
    "        \"home_path\": \"~/.sparkmagic\"\n",
    "      }\n",
    "    },\n",
    "    \"loggers\": {\n",
    "      \"magicsLogger\": { \n",
    "        \"handlers\": [\"magicsHandler\"],\n",
    "        \"level\": \"DEBUG\",\n",
    "        \"propagate\": 0\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "\n",
    "  \"wait_for_idle_timeout_seconds\": 15,\n",
    "  \"livy_session_startup_timeout_seconds\": 60,\n",
    "\n",
    "  \"fatal_error_suggestion\": \"The code failed because of a fatal error:\\n\\t{}.\\n\\nSome things to try:\\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\\nc) Restart the kernel.\",\n",
    "\n",
    "  \"ignore_ssl_errors\": false,\n",
    "\n",
    "  \"session_configs\": {\n",
    "    \"driverMemory\": \"1000M\",\n",
    "    \"executorCores\": 2\n",
    "  },\n",
    "\n",
    "  \"use_auto_viz\": true,\n",
    "  \"coerce_dataframe\": true,\n",
    "  \"max_results_sql\": 2500,\n",
    "  \"pyspark_dataframe_encoding\": \"utf-8\",\n",
    "  \n",
    "  \"heartbeat_refresh_seconds\": 30,\n",
    "  \"livy_server_heartbeat_timeout_seconds\": 0,\n",
    "  \"heartbeat_retry_seconds\": 10,\n",
    "\n",
    "  \"server_extension_default_kernel_name\": \"pysparkkernel\",\n",
    "  \"custom_headers\": {},\n",
    "  \n",
    "  \"retry_policy\": \"configurable\",\n",
    "  \"retry_seconds_to_sleep_list\": [0.2, 0.5, 1, 3, 5],\n",
    "  \"configurable_retry_policy_max_retries\": 8\n",
    "}\n",
    "```\n",
    "\n",
    "我们一般只用修改不同kernel中的url就可以进行配置了.另一个比较重要的参数是`wait_for_idle_timeout_seconds`和`livy_session_startup_timeout_seconds`,它们管着过期时间.可能会需要微调.\n",
    "\n",
    "每次启动一个sparkmagic的notebook后使用`sc`进行初始化.这会创建一个livy-session,同时也创建一个spark的会话上下文.sc这个变量就是这个spark上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<SparkContext master=local appName=livy-session-6>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparkmagic提供了一些魔术方法方便我们管理spark任务.可以使用`%%help`来查看有哪些魔术方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用于分析的数据一般会放在这么几个位置:\n",
    "\n",
    "+ `hdfs`\n",
    "+ 对象存储,亚马逊有对象存储s3,它在本地可以使用[aws-cli](https://github.com/aws/aws-cli)或者[Boto 3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)进行访问和操作\n",
    "\n",
    "这两种方式都可以直接安装[smart_open](https://github.com/RaRe-Technologies/smart_open)在本地操作,然后在spark上使用read命令操作\n",
    "\n",
    "+ `hive` 如果spark已经配置好了hive连接,那么可以使用全局变量`sqlContext`来访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(np.random.rand(100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2\n",
      "0  0.043678  0.931353  0.335554\n",
      "1  0.741314  0.419054  0.830939\n",
      "2  0.631265  0.369277  0.705801\n",
      "3  0.919189  0.532052  0.250095\n",
      "4  0.861283  0.711060  0.763949\n",
      "5  0.605339  0.282307  0.552550\n",
      "6  0.321202  0.111927  0.464540\n",
      "7  0.413631  0.190857  0.855158\n",
      "8  0.311511  0.864287  0.367467\n",
      "9  0.852249  0.743370  0.415710"
     ]
    }
   ],
   "source": [
    "pdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50"
     ]
    }
   ],
   "source": [
    "df[df[0]>0.5].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "\n",
    "数据分析,模型训练都是建立在有数据的基础上的,一般我们的数据来源有这个几个方面:\n",
    "\n",
    "+ 数据库,一般是mysql,pg这些关系数据库\n",
    "+ 共享内存,以redis为代表的共享内存一般用于缓存热数据.\n",
    "+ kafka,通常用于流处理\n",
    "+ hive 最常见的数据仓库\n",
    "+ hdfs 以文件的形式保存数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取mysql数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_df = spark.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://172.16.1.77:3306/samh\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"(SELECT * FROM cartoon) tmp\", user=\"recommend\",\n",
    "    password=\"R1QaL1^G0Fpv8^ZT\"\n",
    ").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101639"
     ]
    }
   ],
   "source": [
    "mysql_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_pdf = mysql_df.sample(False, fraction=0.01).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cartoon_id cartoon_name  ... uploader_Uid uploader_Uname\n",
      "0         113        70亿之针  ...         1971          张洁80后\n",
      "1         130        稻荷恋之歌  ...         1971          张洁80后\n",
      "2         259         花的名字  ...         1977           扬州酒版\n",
      "3         262        美女是野兽  ...         1977           扬州酒版\n",
      "4         265      汤尼岳崎的钢弹  ...         1977           扬州酒版\n",
      "5         425         REAL  ...         1985         紫藤花恋98\n",
      "6         529         欺诈游戏  ...         2618      爱听歌的牛牛爱大齐\n",
      "7         607          秦始皇  ...         2284           万度HR\n",
      "8         715         野蛮人乔  ...         1998          小炎865\n",
      "9         767       魔兽世界漫画  ...         2000        跟着时尚感觉走\n",
      "\n",
      "[10 rows x 76 columns]"
     ]
    }
   ],
   "source": [
    "mysql_pdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sample in module pyspark.sql.dataframe:\n",
      "\n",
      "sample(withReplacement=None, fraction=None, seed=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a sampled subset of this :class:`DataFrame`.\n",
      "    \n",
      "    :param withReplacement: Sample with replacement or not (default False).\n",
      "    :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n",
      "    :param seed: Seed for sampling (default a random seed).\n",
      "    \n",
      "    .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      "        count of the given :class:`DataFrame`.\n",
      "    \n",
      "    .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      "    \n",
      "    >>> df = spark.range(10)\n",
      "    >>> df.sample(0.5, 3).count()\n",
      "    4\n",
      "    >>> df.sample(fraction=0.5, seed=3).count()\n",
      "    4\n",
      "    >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      "    1\n",
      "    >>> df.sample(1.0).count()\n",
      "    10\n",
      "    >>> df.sample(fraction=1.0).count()\n",
      "    10\n",
      "    >>> df.sample(False, fraction=1.0).count()\n",
      "    10\n",
      "    \n",
      "    .. versionadded:: 1.3"
     ]
    }
   ],
   "source": [
    "help(mysql_df.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_df.write.format(\"jdbc\").options(url=\"jdbc:mysql://172.16.1.77:3306/test\",\n",
    "                                      driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "                                      dbtable=\"spark_cartoon\", user=\"recommend\",\n",
    "                                      password=\"R1QaL1^G0Fpv8^ZT\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取本地文件写入hdfs\n",
    "\n",
    "很遗憾,这个操作是不行的,我们可以做的不过是读出数据,转成dict,然后拷贝到spark而已.更靠谱的方式是将文件传入hdfs或者对象存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'hsz', 'age': 18}, {'name': 'hzj', 'age': 20}, {'name': 'zyf', 'age': 28}, {'name': 'ykl', 'age': 38}]\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import json\n",
    "with open(\"data/user.json\") as f:\n",
    "    localjson = json.load(f)\n",
    "print(localjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Path does not exist: hdfs://iZbp13z41dx386bju3wb1zZ:8020/user/huangsizhe/userdata.parquet;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/spark/spark-2.4.0-bin-2.6.0-cdh5.7.0/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 316, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "  File \"/data/spark/spark-2.4.0-bin-2.6.0-cdh5.7.0/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/data/spark/spark-2.4.0-bin-2.6.0-cdh5.7.0/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Path does not exist: hdfs://iZbp13z41dx386bju3wb1zZ:8020/user/huangsizhe/userdata.parquet;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"userdata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame([{'name': 'hsz', 'age': 18}, {'name': 'hzj', 'age': 20}, {'name': 'zyf', 'age': 28}, {'name': 'ykl', 'age': 38}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age name\n",
      "0   18  hsz\n",
      "1   20  hzj\n",
      "2   28  zyf\n",
      "3   38  ykl"
     ]
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o107.parquet.\n",
      ": org.apache.hadoop.security.AccessControlException: Permission denied: user=huangsizhe, access=WRITE, inode=\"/user\":root:supergroup:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6590)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6572)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6524)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4322)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4292)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:867)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:322)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:603)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3084)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3049)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:957)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:953)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:953)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:946)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1861)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:305)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:139)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=huangsizhe, access=WRITE, inode=\"/user\":root:supergroup:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6590)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6572)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6524)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4322)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4292)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:867)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:322)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:603)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1471)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1408)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n",
      "\tat com.sun.proxy.$Proxy25.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:544)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n",
      "\tat com.sun.proxy.$Proxy26.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3082)\n",
      "\t... 43 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/spark/spark-2.4.0-bin-2.6.0-cdh5.7.0/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 841, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/data/spark/spark-2.4.0-bin-2.6.0-cdh5.7.0/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/data/spark/spark-2.4.0-bin-2.6.0-cdh5.7.0/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/data/spark/spark-2.4.0-bin-2.6.0-cdh5.7.0/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o107.parquet.\n",
      ": org.apache.hadoop.security.AccessControlException: Permission denied: user=huangsizhe, access=WRITE, inode=\"/user\":root:supergroup:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6590)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6572)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6524)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4322)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4292)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:867)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:322)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:603)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3084)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3049)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:957)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:953)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:953)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:946)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1861)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:305)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:139)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=huangsizhe, access=WRITE, inode=\"/user\":root:supergroup:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6590)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6572)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6524)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4322)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4292)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:867)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:322)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:603)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1471)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1408)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n",
      "\tat com.sun.proxy.$Proxy25.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:544)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n",
      "\tat com.sun.proxy.$Proxy26.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3082)\n",
      "\t... 43 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"hdfs://iZbp13z41dx386bju3wb1zZ:8020/user/huangsizhe/userdata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
